{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7011378",
   "metadata": {},
   "source": [
    "NLP Project for KU Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecaf78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Load the dataset\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c03e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove unwanted characters from the questions\n",
    "\n",
    "def cleanDf(df):\n",
    "    \n",
    "    pattern = re.compile(r\"[?؟,;\\/\\\\\\[\\]#():]\")\n",
    "    df['question'] = df['question'].apply(lambda x: pattern.sub(\"\", x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "feffa14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: ar, Train Questions: 2558, Validation Questions: 415\n",
      "Language: ar, Train Total Words: 16202, Validation Total Words: 2621\n",
      "Language: ko, Train Questions: 2422, Validation Questions: 356\n",
      "Language: ko, Train Total Words: 11840, Validation Total Words: 1729\n",
      "Language: te, Train Questions: 1355, Validation Questions: 384\n",
      "Language: te, Train Total Words: 7668, Validation Total Words: 2299\n"
     ]
    }
   ],
   "source": [
    "langForStat = ['ar','ko','te']\n",
    "\n",
    "numQuestions = []\n",
    "totalWordCount = []\n",
    "distinctWordCount = []\n",
    "distinctCharCount = []\n",
    "\n",
    "df_train_clean = cleanDf(df_train)\n",
    "df_train_val = cleanDf(df_val)\n",
    "\n",
    "for lang in langForStat:\n",
    "    numQuestions_train = df_train_clean[df_train_clean['lang'] == lang].shape[0]\n",
    "    numQuestions_val = df_train_val[df_train_val['lang'] == lang].shape[0]\n",
    "    numQuestions.append((lang, numQuestions_train, numQuestions_val))\n",
    "    print(f\"Language: {lang}, Train Questions: {numQuestions_train}, Validation Questions: {numQuestions_val}\")\n",
    "\n",
    "    # Compute word and character statistics\n",
    "    df_train_lang = df_train_clean[df_train_clean['lang'] == lang].copy()\n",
    "    df_val_lang = df_train_val[df_train_val['lang'] == lang]\n",
    "    df_train_lang['wordcount'] = df_train_lang['question'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "\n",
    "    maxId = df_train_lang['wordcount'].idxmax()\n",
    "    longest_question = df_train_lang.loc[maxId, \"question\"]\n",
    "    max_words = df_train_lang.loc[maxId, \"wordcount\"]\n",
    "\n",
    "    # print(f\"Language: {lang}\")\n",
    "    # print(f\"  Longest train question (index {maxId}): {longest_question}\")\n",
    "    # print(f\"  Word count: {max_words}\")\n",
    "\n",
    "    totalWordCount_train = df_train_lang['question'].apply(lambda x: len(x.split())).sum()\n",
    "    totalWordCount_val = df_val_lang['question'].apply(lambda x: len(x.split())).sum()\n",
    "    totalWordCount.append((lang, totalWordCount_train, totalWordCount_val))\n",
    "    print(f\"Language: {lang}, Train Total Words: {totalWordCount_train}, Validation Total Words: {totalWordCount_val}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "89d6d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: ar, Distinct Words: 1071\n",
      "Language: ar, Distinct Characters: 52\n",
      "Language: ar, Calculated Total Words from Distinct Words: 1808\n",
      "Language: ar, Top 5 Words: [('هل', 247), ('في', 80), ('من', 52), ('يمكن', 31), ('على', 18), ('هو', 13), ('يوجد', 11), ('ان', 9), ('توجد', 9), ('يعتبر', 9)]\n",
      "Language: ko, Distinct Words: 279\n",
      "Language: ko, Distinct Characters: 309\n",
      "Language: ko, Calculated Total Words from Distinct Words: 325\n",
      "Language: ko, Top 5 Words: [('수', 11), ('있는가', 10), ('있을까', 8), ('승리했나요', 2), ('땅과', 2), ('물', 2), ('둘', 2), ('다에서', 2), ('살', 2), ('범주에', 2)]\n",
      "Language: te, Distinct Words: 134\n",
      "Language: te, Distinct Characters: 55\n",
      "Language: te, Calculated Total Words from Distinct Words: 269\n",
      "Language: te, Top 5 Words: [('ఏ', 16), ('ఏది', 10), ('అతిపెద్ద', 8), ('దేశం', 8), ('ఎంత', 7), ('ఎవరు', 7), ('దేశంలో', 5), ('నాటికి', 4), ('ఆఫ్రికాలో', 4), ('జనాభా', 4)]\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def wordCount(df, lang):\n",
    "    allWords = []\n",
    "    df = df[df['lang'] == lang].copy()\n",
    "    \n",
    "    df = df[df['answerable'] == False]\n",
    "    df['question'] = df['question'].astype(str)\n",
    "\n",
    "    for q in df['question']:\n",
    "        allWords.extend(q.split()) \n",
    "    \n",
    "    wordDict = dict(Counter(allWords))\n",
    "    wordDict = dict(sorted(wordDict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return wordDict\n",
    "            \n",
    "\n",
    "for lang in langForStat:\n",
    "    wordDict = wordCount(df_train_clean, lang)\n",
    "    distinctWordCount.append((lang, len(wordDict)))\n",
    "    print(f\"Language: {lang}, Distinct Words: {len(wordDict)}\")\n",
    "\n",
    "    allChars = []\n",
    "    for word in wordDict.keys():\n",
    "        allChars.extend(list(word))\n",
    "    \n",
    "    charDict = dict(Counter(allChars))\n",
    "    charDict = dict(sorted(charDict.items(), key=lambda item: item[1], reverse=True))\n",
    "    distinctCharCount.append((lang, len(charDict)))\n",
    "    print(f\"Language: {lang}, Distinct Characters: {len(charDict)}\")\n",
    "    calculatedTotalWords = sum(wordDict.values())\n",
    "    print(f\"Language: {lang}, Calculated Total Words from Distinct Words: {calculatedTotalWords}\")\n",
    "\n",
    "    top5Words = list(wordDict.items())[:10]\n",
    "    print(f\"Language: {lang}, Top 5 Words: {top5Words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "976793c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic Classifier Accuracy: 88.04%\n"
     ]
    }
   ],
   "source": [
    "def arabicClassifier(sentence):\n",
    "    goodWords = ['متى','ما','هو','هي','كم','عدد','أول','في']\n",
    "    badWords = ['من','هل']\n",
    "    if any(word in sentence for word in goodWords):\n",
    "        return True\n",
    "    elif any(word in sentence for word in badWords):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "## Example usage of the Arabic classifier\n",
    "arabicDf = df_train_clean[df_train_clean['lang'] == 'ar'].copy()\n",
    "arabicDf['prediction'] = arabicDf['question'].apply(arabicClassifier)\n",
    "\n",
    "accuracy = (arabicDf['answerable'] == arabicDf['prediction']).mean()\n",
    "\n",
    "print(f\"Arabic Classifier Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
